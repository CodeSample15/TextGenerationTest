{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ecbc4f",
   "metadata": {},
   "source": [
    "<h1>I hate myself for attempting this but I'm bored</h1>\n",
    "<br></br>\n",
    "<h3>Let me explain myself because I feel this needs explaining:</h3>\n",
    "<p>Recently, the GPT model has dominated the text-generation realm of AI (as it should, it's fucking amazing). However, I want to see how far I can push the RNN model for text generation before it can't be pushed any further. Of course, conditional text generation has been done before in the past with RNN, and very well, BUT: I am stubborn as fuck and need to experiment by myself to see what I can do. Also I just finished working on a video game and need a not so overcomplicated project to suck me back into the world of AI. I'll document my findings here in this notebook so you can see the deterioration of my sanity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "daa4505f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T05:01:45.980964Z",
     "start_time": "2023-08-30T05:01:45.960965Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import requests\n",
    "\n",
    "DESC_FILE = '../ep_descs.pkl'\n",
    "TRANS_FILE = 'D:/Machine Learning/Datasets/SpongeBob_SquarePants_Transcripts/'\n",
    "\n",
    "MAX_SEQ_LEN = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be69ec7",
   "metadata": {},
   "source": [
    "<h2>Problem #1: web scraping</h2>\n",
    "<br></br>\n",
    "<p>In order to get the generator to generate scripts from a prompt, I need to get descriptions of each episode (which will be the prompts of this generator). Fortunately, this data can be found on the Spongebob Wiki. Unfortunately, this means I have to do some mother-fucking web scraping. I'll try explain what I'm doing with comments but to be honest, I'm going to give up on that pretty quickly. Sorry in advance for my ugly python code</p>\n",
    "<p>For ease of use later on, I'll store the data (which will be kept in a dictionary) into a seperate file using python's pickle</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2417651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T04:50:46.812957Z",
     "start_time": "2023-08-30T04:50:42.660802Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#running for each season of spongebob\n",
    "SEASONS = 13\n",
    "data = {}\n",
    "\n",
    "for season in range(1, SEASONS+1):\n",
    "    page = requests.get(f\"https://spongebob.fandom.com/wiki/Season_{season}#List_of_episodes\").content\n",
    "    \n",
    "    soup = bs4(page, 'html.parser')\n",
    "    table = soup.find_all('table', class_='general')[0]\n",
    "        \n",
    "    #let's get the title and descriptions of each episode\n",
    "    titles = table.find_all('td', style='text-align:left')\n",
    "    descriptions = table.find_all('td', colspan='4')\n",
    "    \n",
    "    for j in range(len(titles)):\n",
    "        title = titles[j].text\n",
    "        desc = descriptions[j].text\n",
    "        #getting rid of the quotation marks around the episode name with possibly the most python line of code ever written (then also do some other pre processing)\n",
    "        title = title[1:-2]\n",
    "        title = title.replace(' ', '')\n",
    "        title = title.lower()\n",
    "        \n",
    "        data[title] = desc #updating the dataset\n",
    "        \n",
    "#save the compiled data to a file\n",
    "with open(DESC_FILE, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceeadb8",
   "metadata": {},
   "source": [
    "<p>Alright, that wasn't as bad as I thought it would be. I ended up getting the script to work on the third or fourth time. Luckily the spongebob wiki doesn't require javascript to load its shit, so I didn't have to boot up selenium to get this scraper to work. Come to think of it, this was probably the best experience I've had writing a web scraper. Nice</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18a47c",
   "metadata": {},
   "source": [
    "<h1>Problem #2: bringing both datasets together</h1>\n",
    "<br></br>\n",
    "<p>Now that I have all the data I need, I need to compile everything into a single dataset that I can use to train the network with. This means for the transcripts, I have to clean, tokenize, and pad everything. For the episode descriptions, I need to combine the descriptions with the correct transcripts.</p>\n",
    "<p>Some things to consider: RNN models are notoriously bad at generating long sequences of text. Should I set the sampling size lower so that the model performs better? Or should I keep it high so I can really test the limits of this type of model. Hmmmm.... I guess I'll figure it out as I go, but I'm pretty sure I'm going to have to set the sampling size to be smaller. I'll start with the full transcripts and go from there. I think the main reason I want to do this is because I already know how bad LSTM is at performing this task, but I haven't really tested the GRU layer that much and am curious as to how it performs. I think I'll end up training two models.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de593a2b",
   "metadata": {},
   "source": [
    "<h3>Quick side note:</h3>\n",
    "<p>I want to really quickly go over in text what I want the architecture of the model to be (inputs and outputs for now, I'll go into more detail when I build the model). For starters, the model will have two inputs: X1 and X2. They will store the previous text in the transcript and the prompt, respectively. The output will be a one-hot encoding of the next token to be added to the sequence. My hope with using the two seperate inputs for the model will help it perform better when generating scripts that are more relevant to the provided episode prompt. Again, GPT models handle this problem with ease, but when it comes to RNNs, I want to make sure the model remembers what it should be generating</p>\n",
    "<p>I'll go with this design for as long as possible, so hopefully I don't lose my shit trying to keep this plan from failing. I have to remember that this is not going to work the first time cuz I have no clue what the fuck I'm doing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc50dd7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T04:50:53.457310Z",
     "start_time": "2023-08-30T04:50:46.814956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 items failed to load\n",
      "317 items loaded!\n"
     ]
    }
   ],
   "source": [
    "#let's start with the basics: loading the data from disk\n",
    "dataset = {}\n",
    "\n",
    "descs = {}\n",
    "with open(DESC_FILE, 'rb') as f:\n",
    "    descs = pickle.load(f)\n",
    "\n",
    "failed_items = 0\n",
    "successful_items = 0\n",
    "    \n",
    "for file in glob.glob(TRANS_FILE + '*.txt'):\n",
    "    ep_title = file.split('\\\\')[1].split('.')[0]\n",
    "    ep_title = ep_title.lower()\n",
    "    \n",
    "    #try to access the description of the episode, if it's not on file, just ignore it\n",
    "    try:\n",
    "        desc = descs[ep_title]\n",
    "        \n",
    "        lines = []\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                lines.append(line.lower())\n",
    "                #lines.extend('\\n')\n",
    "        \n",
    "        dataset[desc] = lines\n",
    "        successful_items += 1\n",
    "    except:\n",
    "        failed_items += 1\n",
    "        continue\n",
    "\n",
    "print(f\"{failed_items} items failed to load\")\n",
    "print(f\"{successful_items} items loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f6959a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T05:02:18.707116Z",
     "start_time": "2023-08-30T05:01:51.744438Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#alright, now let's process and organize this data (spaghetti code incoming)\n",
    "print(\"Processing data...\")\n",
    "\n",
    "pretokenized = []\n",
    "pretokenizedDescs = []\n",
    "\n",
    "def tokenize(line):\n",
    "    #not even going to explain this mess. Just know this cleans the data. Don't currently know a better way of doing this and it's killing me inside. \n",
    "    tokens = []\n",
    "    for char in line:\n",
    "        tokens.append(char)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_words(line):\n",
    "    #not even going to explain this mess. Just know this cleans the data. Don't currently know a better way of doing this and it's killing me inside. \n",
    "    tokens = []\n",
    "    for word in line.split(' '):\n",
    "        newword = \"\"\n",
    "        for char in word:\n",
    "            if not char.isalpha():\n",
    "                if newword != \"\":\n",
    "                    tokens.append(newword)\n",
    "                newword = \"\"\n",
    "                tokens.append(char)\n",
    "            else:\n",
    "                newword += char\n",
    "        if(newword!=''):\n",
    "            tokens.append(newword)\n",
    "    return tokens\n",
    "\n",
    "for desc in dataset:\n",
    "    #clean the data\n",
    "    tempdata = [x for x in dataset[desc] if x != '\\n']\n",
    "    tokens = []\n",
    "    descTokens = []\n",
    "    \n",
    "    for line in tempdata:\n",
    "        line += \"\\n\"\n",
    "        tokens.extend(tokenize_words(line))\n",
    "        \n",
    "    pretokenizedDescs.append(tokenize_words(desc.lower()))\n",
    "    #removing unecessary tokens\n",
    "    pretokenizedDescs[-1].pop(pretokenizedDescs[-1].index('\\n'))\n",
    "    \n",
    "    pretokenized.append(tokens)\n",
    "    \n",
    "#now let's tokenize this sucker and then move on to building the dataset to be fed to the network.\n",
    "lib = [''] #library of tokens\n",
    "tokenized = [] #dataset tokenized\n",
    "tokenizedDescs = [] #same thing but descriptions\n",
    "maxLen = 0 #for padding later down the road\n",
    "maxDescLen = 0 #same thing but for descriptions\n",
    "\n",
    "#first the transcripts (calculating the maximum sequence length and then tokenizing the dataset using the lib array)\n",
    "for i in pretokenized:\n",
    "    if len(i) > maxLen:\n",
    "        maxLen = len(i)\n",
    "    for j in i:\n",
    "        if j not in lib:\n",
    "            lib.append(j)\n",
    "            \n",
    "for i in pretokenized:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(lib.index(j))\n",
    "    tokenized.append(temp)\n",
    "    \n",
    "#now the descriptions\n",
    "for i in pretokenizedDescs:\n",
    "    if len(i) > maxDescLen:\n",
    "        maxDescLen = len(i)\n",
    "    for j in i: #continuing to build upon the library just in case there are some tokens that didn't make the list yet\n",
    "        if j not in lib:\n",
    "            lib.append(j)\n",
    "            \n",
    "for i in pretokenizedDescs:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(lib.index(j))\n",
    "    tokenizedDescs.append(temp)\n",
    "    \n",
    "#now we need to pad the tokenized descriptions since that won't happen automatically when the dataset it built\n",
    "for i, n in enumerate(tokenizedDescs):\n",
    "    for j in range(maxDescLen-len(n)):\n",
    "        tokenizedDescs[i].append(0) #padding (0 token represents and empty string in the lib array)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce3b6b91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T05:02:18.722126Z",
     "start_time": "2023-08-30T05:02:18.709117Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "#smallest length a sequence can be\n",
    "MIN_LEN = MAX_SEQ_LEN\n",
    "\n",
    "X1 = [] #ep descriptions\n",
    "X2 = [] #prev tokens\n",
    "Y = []  #ep transcipts\n",
    "\n",
    "#helper to generate random batches of data from the main dataset\n",
    "#loading the whole dataset to memory at one time on my 16GB of ram would fry my computer pretty quickly. Let's avoid that\n",
    "def get_dataset(batch_size, size=MAX_SEQ_LEN):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    Y = []\n",
    "    for batch in range(batch_size):\n",
    "        rand_trans = random.randrange(MIN_LEN, len(tokenized)) #get a random transcript to get information from\n",
    "        rand_start = random.randrange(0, len(tokenized[rand_trans])-size)\n",
    "        \n",
    "        temp = tokenized[rand_trans][rand_start:rand_start+size]\n",
    "\n",
    "        X1.append(np.array(temp))\n",
    "        X2.append(tokenizedDescs[rand_trans])\n",
    "        Y.append(tokenized[rand_trans][rand_start+size])\n",
    "        \n",
    "    X1 = np.array(X1, dtype='float32')\n",
    "    X2 = np.array(X2, dtype='float32')\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    Y = to_categorical(Y, num_classes=len(lib))\n",
    "    return X1, X2, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f7ac2",
   "metadata": {},
   "source": [
    "<h1>Problem #3: Building the model</h1>\n",
    "<br></br>\n",
    "<p>Gettting the data prepped is always the most annoying and difficult part. This time (for me at least) was no exception. I spent a good 2 hours writing the above cells, and now I'm ready to move on. Luckily, with everything put into place, it should IN THEORY be easy to set this model up.</p>\n",
    "<p>I already went over the plan in an above cell, so I won't explain myself again. This is already a stupid idea, I'm just going to roll with it at this point. I'm going to start with one GRU layer for each input layer, and then move on from there depending on how the model performs. I'll make sure to update this cell if I decide to change the model design drastically (which I most likely will do). I also plan on comparing the difference in performance between a LSTM model and a GRU model, so I'll document my findings later down the line. I feel like GRU will perform better, but I'll test both models just to be sure.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e583fb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T05:13:26.452104Z",
     "start_time": "2023-08-30T05:13:26.432080Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GRU, LSTM, Dropout, BatchNormalization, concatenate, TimeDistributed, Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def GRUModel():\n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN, len(lib)))\n",
    "    \n",
    "    x = GRU(64, return_sequences=True)(x_in)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    #description input\n",
    "    y_in = Input(shape=(MAX_SEQ_LEN, len(lib)))\n",
    "    \n",
    "    y = GRU(32, return_sequences=True)(y_in)\n",
    "    y = LeakyReLU()(y)\n",
    "    \n",
    "    y = Model(inputs=y_in, outputs=y)\n",
    "    \n",
    "    #combine\n",
    "    z = concatenate([x.output, y.output])\n",
    "    \n",
    "    z = TimeDistributed(Dense(128))(z)\n",
    "    z = TimeDistributed(LeakyReLU())(z)\n",
    "    z = TimeDistributed(Dropout(0.2))(z)\n",
    "    \n",
    "    z = Flatten()(z)\n",
    "    \n",
    "    z = Dense(len(lib), activation='softmax')(z)\n",
    "    \n",
    "    z = Model(inputs=[x_in, y_in], outputs=z)\n",
    "    return z\n",
    "\n",
    "def TestModel():\n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN))\n",
    "    \n",
    "    x = Embedding(len(lib), 70, input_length=1)(x_in)\n",
    "    \n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "    x = GRU(64)(x)\n",
    "    \n",
    "    x = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    y_in = Input(shape=(maxDescLen))\n",
    "    \n",
    "    y = Embedding(len(lib), 70, input_length=1)(y_in)\n",
    "    \n",
    "    y = GRU(128)(y)\n",
    "    y = Dropout(0.3)(y)\n",
    "    \n",
    "    y = Model(inputs=y_in, outputs=y)\n",
    "    \n",
    "    z = concatenate([x.input, y.input])\n",
    "    \n",
    "    z = Dense(128)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    \n",
    "    z = Dense(len(lib), activation='softmax')(z)\n",
    "    \n",
    "    z = Model(inputs=[x_in, y_in], outputs=z)\n",
    "    return z\n",
    "\n",
    "def LSTMModel():\n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN))\n",
    "    \n",
    "    x = Embedding(len(lib), 20, input_length=1)(x_in)\n",
    "    x = LSTM(300)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(200)(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    \n",
    "    x = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    #description input\n",
    "    y_in = Input(shape=(maxDescLen))\n",
    "    \n",
    "    y = Embedding(len(lib), 20, input_length=1)(y_in)\n",
    "    y = LSTM(300)(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    \n",
    "    y = Dense(200)(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    \n",
    "    y = Model(inputs=y_in, outputs=y)\n",
    "    \n",
    "    #combine\n",
    "    z = concatenate([x.output, y.output])\n",
    "    \n",
    "    z = Dense(128, activation='relu')(z)\n",
    "    \n",
    "    z = Dense(len(lib), activation='softmax')(z)\n",
    "    \n",
    "    z = Model(inputs=[x_in, y_in], outputs=z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bb8646ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T05:13:28.967595Z",
     "start_time": "2023-08-30T05:13:28.551347Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 20)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 39)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 59)           0           input_4[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 128)          7680        concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 128)          0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 17867)        2304843     dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 2,312,523\n",
      "Trainable params: 2,312,523\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = TestModel()\n",
    "\n",
    "opt = Adam(learning_rate=0.0003)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5921be10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T05:22:41.582017Z",
     "start_time": "2023-08-30T05:18:27.106488Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10000/10000: LOSS=6.017171859741211    ACC=0.1015625\n",
      "Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x17fb3c856d0>,\n",
       " <matplotlib.lines.Line2D at 0x17fb3c85730>]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fitting the model (please god please work please)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "epochs = 10000\n",
    "batch_size = 128\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    #generate some data\n",
    "    X1, X2, Y = get_dataset(batch_size, MAX_SEQ_LEN)\n",
    "\n",
    "    #train the network\n",
    "    loss = model.train_on_batch([X1, X2], Y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch {i+1}/{epochs}: LOSS={loss[0]}    ACC={loss[1]}\")\n",
    "\n",
    "print(\"Finished\")\n",
    "model.save(\"../SpongeBobLSTM.h5\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e193c75",
   "metadata": {},
   "source": [
    "<h1>Problem #4: Testing the model</h1>\n",
    "<br></br>\n",
    "<p>Now that I have a trainable model, I need a way of testing it's capabilities outside of a graph. My goal is to create a program that takes a prompt, tokenizes and processes the prompt, and then passes it through the model until the model generates a story as big as what it can take as input (if that makes any sense). Hopefully the code should explain itself for this one as I'll be using a lot of the code I already written.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c0fbb8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T05:12:21.000034Z",
     "start_time": "2023-08-30T05:12:15.588304Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter prompt for episode: Squidward dies\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed4ea75ca40145c9ba631131831bf984",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1525 predict_step\n        return self(x, training=False)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 1 is incompatible with layer model_3: expected shape=(None, 39), found shape=(None, 20)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m counter \u001b[38;5;241m=\u001b[39m start_seq_len\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m log_progress(\u001b[38;5;28mrange\u001b[39m(MAX_SEQ_LEN\u001b[38;5;241m-\u001b[39mstart_seq_len)):\n\u001b[1;32m---> 34\u001b[0m     next_token \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mep_prompt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m     episode[\u001b[38;5;241m0\u001b[39m][counter] \u001b[38;5;241m=\u001b[39m next_token[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margmax()\n\u001b[0;32m     36\u001b[0m     counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1727\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1725\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39msteps():\n\u001b[0;32m   1726\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m-> 1727\u001b[0m   tmp_batch_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1728\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1729\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:889\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 889\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    891\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    892\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    936\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:763\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_internal_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    767\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3050\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 3050\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3444\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3440\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3441\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3444\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3279\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3274\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3276\u001b[0m ]\n\u001b[0;32m   3277\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3278\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3282\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3287\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3288\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3291\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3292\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3293\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3294\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3295\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3296\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:999\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    997\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m--> 999\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1004\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:672\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    669\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    670\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    671\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 672\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:986\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    985\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    987\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1569 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1559 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1552 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1525 predict_step\n        return self(x, training=False)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:267 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) +\n\n    ValueError: Input 1 is incompatible with layer model_3: expected shape=(None, 39), found shape=(None, 20)\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm as log_progress\n",
    "\n",
    "#how many tokens to take from the actual dataset \n",
    "start_seq_len = 5\n",
    "\n",
    "prompt = input(\"Enter prompt for episode: \")\n",
    "prompt = prompt.lower()\n",
    "\n",
    "#tokenize the prompt if it's short enough\n",
    "if len(prompt.split(' ')) < MAX_SEQ_LEN:\n",
    "    tokens = tokenize(prompt)\n",
    "    ep_prompt = []\n",
    "    \n",
    "    for i in tokens:\n",
    "        if i in lib:\n",
    "            ep_prompt.append(lib.index(i))\n",
    "        else:\n",
    "            ep_prompt.append(0)\n",
    "            \n",
    "    for i in range(MAX_SEQ_LEN-len(ep_prompt)):\n",
    "        ep_prompt.append(0)\n",
    "        \n",
    "    ep_prompt = np.array(ep_prompt)\n",
    "    ep_prompt = np.reshape(ep_prompt, (1, ep_prompt.shape[0]))\n",
    "    \n",
    "    #create a temp array to hold the previous tokens of the generated story\n",
    "    episode = np.zeros((1, MAX_SEQ_LEN))\n",
    "    episode[0][0:start_seq_len] = random.choice(tokenized)[0:start_seq_len]\n",
    "    \n",
    "    episode[0][0] = random.randrange(0, len(lib)) #start with a random token\n",
    "    \n",
    "    counter = start_seq_len\n",
    "    for i in log_progress(range(MAX_SEQ_LEN-start_seq_len)):\n",
    "        next_token = model.predict([episode, ep_prompt], verbose=0)\n",
    "        episode[0][counter] = next_token[0].argmax()\n",
    "        counter += 1\n",
    "        \n",
    "    #finally print the finished story\n",
    "    episode_script = ''\n",
    "    for i in episode[0]:\n",
    "        episode_script += lib[int(i)] + ' '\n",
    "        \n",
    "    print(episode_script)\n",
    "else:\n",
    "    print(\"Prompt is too long!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407aed4",
   "metadata": {},
   "source": [
    "<h2>Some notes:</h2>\n",
    "<ol>\n",
    "    <li>Embedding layer might not be that useful. I'm just trying to understand and generate sequences, not get the AI to understand the difference between words.</li>\n",
    "    <li>If embedding layers are useless, switching to a character based tokenization might be better than the word based tokenization method I'm using now.</li>\n",
    "    <li>The AI currently does not seem to understand any differences. I've changed the architecture of the model several times with no change in it's performance, so I'm going to guess the problem lies with how I'm feeding and prepping the data.</li>\n",
    "</ol>\n",
    "<br></br>\n",
    "<h3>Performance notes after changing data to character-based tokenization:</h3>\n",
    "<ul>\n",
    "    <li>Doesn't seem to be any change in performance, but I just started with this new strategy, so I'm going to put a little more work into it.</li>\n",
    "    <li>Not sure if it was the character based tokenization or the fact that the model isn't training on empty inputs anymore (no more 0 padding in training data), but the model is now actually performing better. I also simplified the model a bit to only take one input (previous tokens) in order to debug the lack of learning, which also could contribute to this boost in performance. I'm going to add back the second input with this new method and see if there's any changes.</li>\n",
    "    <li>It seems to be the fact that input data is no longer padded that the model is performing better than ever. I am going to try and go back to the old tokenization method to see if that will change anything.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Notes after changing back to word-based tokenization:</h3>\n",
    "<ul>\n",
    "    <li>I included the loss (blue) and accuracy (orange) graph below:</li>\n",
    "    <img src='Graph1.png'>\n",
    "    <li>As you can see, the training is unstable as fuck, but it is slowly getting better over time, which is good. I'm going to add back the second input as well as testing the model with what it can currently produce. After making those changes, I got this graph:</li>\n",
    "    <img src='Graph2.png'>\n",
    "    <li>No idea what the fuck happened here. Amazing how the loss started so high at first, but that might be just from the extra neurons I threw into the model. To make sure the first input isn't the only part of the model training, I'll add some dropout to both inputs and also mess with the Embedding layers for both inputs.</li>\n",
    "    <li>So this is disappointing:</li>\n",
    "    <img src='Graph3.png'>\n",
    "    <li>It seems that the model is no longer improving after training some more... This is annoying.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa52b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
