{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ecbc4f",
   "metadata": {},
   "source": [
    "<h1>I hate myself for attempting this but I'm bored</h1>\n",
    "<br></br>\n",
    "<h3>Let me explain myself because I feel this needs explaining:</h3>\n",
    "<p>Recently, the GPT model has dominated the text-generation realm of AI (as it should, it's fucking amazing). However, I want to see how far I can push the RNN model for text generation before it can't be pushed any further. Of course, conditional text generation has been done before in the past with RNN, and very well, BUT: I am stubborn as fuck and need to experiment by myself to see what I can do. Also I just finished working on a video game and need an overcomplicated project to suck me back into the world of AI. I'll document my findings here in this notebook so you can see the deterioration of my sanity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daa4505f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T04:53:39.832872Z",
     "start_time": "2023-08-25T04:53:38.874909Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import requests\n",
    "\n",
    "DESC_FILE = 'ep_descs.pkl'\n",
    "TRANS_FILE = 'D:/Machine Learning/Datasets/SpongeBob_SquarePants_Transcripts/'\n",
    "\n",
    "MAX_SEQ_LEN = 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be69ec7",
   "metadata": {},
   "source": [
    "<h2>Problem #1: web scraping</h2>\n",
    "<br></br>\n",
    "<p>In order to get the generator to generate scripts from a prompt, I need to get descriptions of each episode (which will be the prompts of this generator). Fortunately, this data can be found on the Spongebob Wiki. Unfortunately, this means I have to do some mother-fucking web scraping. I'll try explain what I'm doing with comments but to be honest, I'm going to give up on that pretty quickly. Sorry in advance for my ugly python code</p>\n",
    "<p>For ease of use later on, I'll store the data (which will be kept in a dictionary) into a seperate file using python's pickle</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2417651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T02:14:57.532665Z",
     "start_time": "2023-08-24T02:14:53.478006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#running for each season of spongebob\n",
    "SEASONS = 13\n",
    "data = {}\n",
    "\n",
    "for season in range(1, SEASONS+1):\n",
    "    page = requests.get(f\"https://spongebob.fandom.com/wiki/Season_{season}#List_of_episodes\").content\n",
    "    \n",
    "    soup = bs4(page, 'html.parser')\n",
    "    table = soup.find_all('table', class_='general')[0]\n",
    "        \n",
    "    #let's get the title and descriptions of each episode\n",
    "    titles = table.find_all('td', style='text-align:left')\n",
    "    descriptions = table.find_all('td', colspan='4')\n",
    "    \n",
    "    for j in range(len(titles)):\n",
    "        title = titles[j].text\n",
    "        desc = descriptions[j].text\n",
    "        #getting rid of the quotation marks around the episode name with possibly the most python line of code ever written (then also do some other pre processing)\n",
    "        title = title[1:-2]\n",
    "        title = title.replace(' ', '')\n",
    "        title = title.lower()\n",
    "        \n",
    "        data[title] = desc #updating the dataset\n",
    "        \n",
    "#save the compiled data to a file\n",
    "with open(DESC_FILE, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceeadb8",
   "metadata": {},
   "source": [
    "<p>Alright, that wasn't as bad as I thought it would be. I ended up getting the script to work on the third or fourth time. Luckily the spongebob wiki doesn't require javascript to load its shit, so I didn't have to boot up selenium to get this scraper to work. Come to think of it, this was probably the best experience I've had writing a web scraper. Nice</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18a47c",
   "metadata": {},
   "source": [
    "<h1>Problem #2: bringing both datasets together</h1>\n",
    "<br></br>\n",
    "<p>Now that I have all the data I need, I need to compile everything into a single dataset that I can use to train the network with. This means for the transcripts, I have to clean, tokenize, and pad everything. For the episode descriptions, I need to combine the descriptions with the correct transcripts.</p>\n",
    "<p>Some things to consider: RNN models are notoriously bad at generating long sequences of text. Should I set the sampling size lower so that the model performs better? Or should I keep it high so I can really test the limits of this type of model. Hmmmm.... I guess I'll figure it out as I go, but I'm pretty sure I'm going to have to set the sampling size to be smaller. I'll start with the full transcripts and go from there. I think the main reason I want to do this is because I already know how bad LSTM is at performing this task, but I haven't really tested the GRU layer that much and am curious as to how it performs. I think I'll end up training two models.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de593a2b",
   "metadata": {},
   "source": [
    "<h3>Quick side note:</h3>\n",
    "<p>I want to really quickly go over in text what I want the architecture of the model to be (inputs and outputs for now, I'll go into more detail when I build the model). For starters, the model will have two inputs: X1 and X2. They will store the previous text in the transcript and the prompt, respectively. The output will be a one-hot encoding of the next token to be added to the sequence. My hope with using the two seperate inputs for the model will help it perform better when generating scripts that are more relevant to the provided episode prompt. Again, GPT models handle this problem with ease, but when it comes to RNNs, I want to make sure the model remembers what it should be generating</p>\n",
    "<p>I'll go with this design for as long as possible, so hopefully I don't lose my shit trying to keep this plan from failing. I have to remember that this is not going to work the first time cuz I have no clue what the fuck I'm doing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc50dd7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T04:53:43.266250Z",
     "start_time": "2023-08-25T04:53:43.177214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 items failed to load\n",
      "303 items loaded!\n"
     ]
    }
   ],
   "source": [
    "#let's start with the basics: loading the data from disk\n",
    "dataset = {}\n",
    "\n",
    "descs = {}\n",
    "with open(DESC_FILE, 'rb') as f:\n",
    "    descs = pickle.load(f)\n",
    "\n",
    "failed_items = 0\n",
    "successful_items = 0\n",
    "    \n",
    "for file in glob.glob(TRANS_FILE + '*.txt'):\n",
    "    ep_title = file.split('\\\\')[1].split('.')[0]\n",
    "    ep_title = ep_title.lower()\n",
    "    \n",
    "    #try to access the description of the episode, if it's not on file, just ignore it\n",
    "    try:\n",
    "        desc = descs[ep_title]\n",
    "        \n",
    "        lines = []\n",
    "        with open(file, 'r') as f:\n",
    "            for line in f.readlines():\n",
    "                lines.append(line.lower())\n",
    "                #lines.extend('\\n')\n",
    "        \n",
    "        dataset[desc] = lines\n",
    "        successful_items += 1\n",
    "    except:\n",
    "        failed_items += 1\n",
    "        continue\n",
    "\n",
    "print(f\"{failed_items} items failed to load\")\n",
    "print(f\"{successful_items} items loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f6959a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T04:54:11.602667Z",
     "start_time": "2023-08-25T04:53:43.784000Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#alright, now let's process and organize this data (spaghetti code incoming)\n",
    "print(\"Processing data...\")\n",
    "\n",
    "pretokenized = []\n",
    "pretokenizedDescs = []\n",
    "\n",
    "def tokenize(line):\n",
    "    #not even going to explain this mess. Just know this cleans the data. Don't currently know a better way of doing this and it's killing me inside. \n",
    "    tokens = []\n",
    "    for word in line.split(' '):\n",
    "        newword = \"\"\n",
    "        for char in word:\n",
    "            if not char.isalpha():\n",
    "                if newword != \"\":\n",
    "                    tokens.append(newword)\n",
    "                newword = \"\"\n",
    "                tokens.append(char)\n",
    "            else:\n",
    "                newword += char\n",
    "        if(newword!=''):\n",
    "            tokens.append(newword)\n",
    "    return tokens\n",
    "\n",
    "for desc in dataset:\n",
    "    #clean the data\n",
    "    tempdata = [x for x in dataset[desc] if x != '\\n']\n",
    "    tokens = []\n",
    "    descTokens = []\n",
    "    \n",
    "    for line in tempdata:\n",
    "        line += \"\\n\"\n",
    "        tokens.extend(tokenize(line))\n",
    "        \n",
    "    pretokenizedDescs.append(tokenize(desc.lower()))\n",
    "    #removing unecessary tokens\n",
    "    pretokenizedDescs[-1].pop(pretokenizedDescs[-1].index('\\n'))\n",
    "    \n",
    "    pretokenized.append(tokens)\n",
    "    \n",
    "#now let's tokenize this sucker and then move on to building the dataset to be fed to the network.\n",
    "lib = [''] #library of tokens\n",
    "tokenized = [] #dataset tokenized\n",
    "tokenizedDescs = [] #same thing but descriptions\n",
    "maxLen = 0 #for padding later down the road\n",
    "maxDescLen = 0 #same thing but for descriptions\n",
    "\n",
    "#first the transcripts (calculating the maximum sequence length and then tokenizing the dataset using the lib array)\n",
    "for i in pretokenized:\n",
    "    if len(i) > maxLen:\n",
    "        maxLen = len(i)\n",
    "    for j in i:\n",
    "        if j not in lib:\n",
    "            lib.append(j)\n",
    "            \n",
    "for i in pretokenized:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(lib.index(j))\n",
    "    tokenized.append(temp)\n",
    "    \n",
    "#now the descriptions\n",
    "for i in pretokenizedDescs:\n",
    "    if len(i) > maxDescLen:\n",
    "        maxDescLen = len(i)\n",
    "    for j in i: #continuing to build upon the library just in case there are some tokens that didn't make the list yet\n",
    "        if j not in lib:\n",
    "            lib.append(j)\n",
    "            \n",
    "for i in pretokenizedDescs:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(lib.index(j))\n",
    "    tokenizedDescs.append(temp)\n",
    "    \n",
    "#now we need to pad the tokenized descriptions since that won't happen automatically when the dataset it built\n",
    "for i, n in enumerate(tokenizedDescs):\n",
    "    for j in range(MAX_SEQ_LEN-len(n)):\n",
    "        tokenizedDescs[i].append(0) #padding (0 token represents and empty string in the lib array)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ce3b6b91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T22:03:44.492184Z",
     "start_time": "2023-08-28T22:03:44.473186Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "#smallest length a sequence can be\n",
    "MIN_LEN = 5\n",
    "\n",
    "X1 = [] #ep descriptions\n",
    "X2 = [] #prev tokens\n",
    "Y = []  #ep transcipts\n",
    "\n",
    "#helper to generate random batches of data from the main dataset\n",
    "#loading the whole dataset to memory at one time on my 16GB of ram would fry my computer pretty quickly. Let's avoid that\n",
    "def get_dataset(batch_size, size=maxLen):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    Y = []\n",
    "    for batch in range(batch_size):\n",
    "        rand_trans = random.randrange(MIN_LEN, len(tokenized)) #get a random transcript to get information from\n",
    "        rand_start = random.randrange(0, min(len(tokenized[rand_trans]), size) - MIN_LEN)\n",
    "        rand_length = random.randrange(1, (min(len(tokenized[rand_trans]), size)) - rand_start)\n",
    "        \n",
    "        temp = tokenized[rand_trans][rand_start:rand_start+rand_length]\n",
    "        for i in range(size-rand_length):\n",
    "            temp.append(0)\n",
    "\n",
    "        X1.append(np.array(temp))\n",
    "        X2.append(tokenizedDescs[rand_trans])\n",
    "        Y.append(tokenized[rand_trans][rand_length])\n",
    "        \n",
    "    X1 = np.array(X1)\n",
    "    X2 = np.array(X2)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    X1 = np.reshape(X1, (X1.shape[0], X1.shape[1]))\n",
    "    X2 = np.reshape(X2, (X2.shape[0], X2.shape[1]))\n",
    "    Y = to_categorical(Y, num_classes=len(lib))\n",
    "    return X1, X2, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f7ac2",
   "metadata": {},
   "source": [
    "<h1>Problem #3: Building the model</h1>\n",
    "<br></br>\n",
    "<p>Gettting the data prepped is always the most annoying and difficult part. This time (for me at least) was no exception. I spent a good 2 hours writing the above cells, and now I'm ready to move on. Luckily, with everything put into place, it should IN THEORY be easy to set this model up.</p>\n",
    "<p>I already went over the plan in an above cell, so I won't explain myself again. This is already a stupid idea, I'm just going to roll with it at this point. I'm going to start with one GRU layer for each input layer, and then move on from there depending on how the model performs. I'll make sure to update this cell if I decide to change the model design drastically (which I most likely will do). I also plan on comparing the difference in performance between a LSTM model and a GRU model, so I'll document my findings later down the line. I feel like GRU will perform better, but I'll test both models just to be sure.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e583fb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T22:04:52.785689Z",
     "start_time": "2023-08-28T22:04:52.758707Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GRU, LSTM, Dropout, BatchNormalization, concatenate, TimeDistributed, Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def GRUModel():\n",
    "    embedding_layer = Embedding(len(lib), 1000, input_length=1)\n",
    "    \n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN))\n",
    "    \n",
    "    x = embedding_layer(x_in)\n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    #description input\n",
    "    y_in = Input(shape=(MAX_SEQ_LEN))\n",
    "    \n",
    "    y = embedding_layer(y_in)\n",
    "    y = GRU(128, return_sequences=True)(y)\n",
    "    y = LeakyReLU()(y)\n",
    "    \n",
    "    y = Model(inputs=y_in, outputs=y)\n",
    "    \n",
    "    #combine\n",
    "    z = concatenate([x.output, y.output])\n",
    "    \n",
    "    z = TimeDistributed(Dense(128))(z)\n",
    "    z = TimeDistributed(LeakyReLU())(z)\n",
    "    z = TimeDistributed(Dropout(0.2))(z)\n",
    "    \n",
    "    z = Flatten()(z)\n",
    "    \n",
    "    z = Dense(len(lib), activation='softmax')(z)\n",
    "    \n",
    "    z = Model(inputs=[x_in, y_in], outputs=z)\n",
    "    return z\n",
    "\n",
    "def TestModel():\n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN, 1))\n",
    "    \n",
    "    x = Embedding(64, 100, input_length=1)(x_in)\n",
    "    \n",
    "    x = GRU(128)(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    \n",
    "    x = Dense(len(lib), activation='softmax')(x)\n",
    "    \n",
    "    z = Model(inputs=x_in, outputs=x)\n",
    "    return z\n",
    "\n",
    "def LSTMModel():\n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN))\n",
    "    \n",
    "    x = Embedding(len(lib), 20, input_length=1)(x_in)\n",
    "    x = LSTM(300)(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    \n",
    "    x = Dense(200)(x)\n",
    "    x = Dropout(0.15)(x)\n",
    "    \n",
    "    x = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    #description input\n",
    "    y_in = Input(shape=(maxDescLen))\n",
    "    \n",
    "    y = Embedding(len(lib), 20, input_length=1)(y_in)\n",
    "    y = LSTM(300)(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    \n",
    "    y = Dense(200)(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    \n",
    "    y = Model(inputs=y_in, outputs=y)\n",
    "    \n",
    "    #combine\n",
    "    z = concatenate([x.output, y.output])\n",
    "    \n",
    "    z = Dense(128, activation='relu')(z)\n",
    "    \n",
    "    z = Dense(len(lib), activation='softmax')(z)\n",
    "    \n",
    "    z = Model(inputs=[x_in, y_in], outputs=z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb8646ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T22:04:59.586801Z",
     "start_time": "2023-08-28T22:04:59.441699Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 250, 95)]         0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 128)               86400     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 95)                12255     \n",
      "=================================================================\n",
      "Total params: 98,655\n",
      "Trainable params: 98,655\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = TestModel()\n",
    "\n",
    "opt = Adam(learning_rate=0.0005)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5921be10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-28T22:05:17.332860Z",
     "start_time": "2023-08-28T22:05:15.841757Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 250, 95) for input KerasTensor(type_spec=TensorSpec(shape=(None, 250, 95), dtype=tf.float32, name='input_3'), name='input_3', description=\"created by layer 'input_3'\"), but it was called on an input with incompatible shape (20, 250).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420 call\n        return self._run_internal_graph(\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:215 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer gru_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (20, 250)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m X1, X2, Y \u001b[38;5;241m=\u001b[39m get_dataset(batch_size, MAX_SEQ_LEN)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#train the network\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(loss)\n\u001b[0;32m     17\u001b[0m clear_output(wait\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1825\u001b[0m, in \u001b[0;36mModel.train_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1821\u001b[0m   iterator \u001b[38;5;241m=\u001b[39m data_adapter\u001b[38;5;241m.\u001b[39msingle_batch_iterator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy, x,\n\u001b[0;32m   1822\u001b[0m                                                 y, sample_weight,\n\u001b[0;32m   1823\u001b[0m                                                 class_weight)\n\u001b[0;32m   1824\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_train_function()\n\u001b[1;32m-> 1825\u001b[0m   logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reset_metrics:\n\u001b[0;32m   1828\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_metrics()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:889\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    886\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    888\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 889\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    891\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    892\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:933\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    931\u001b[0m   \u001b[38;5;66;03m# This is the first call of __call__, so we have to initialize.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m   initializers \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 933\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_initializers_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    934\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    935\u001b[0m   \u001b[38;5;66;03m# At this point we know that the initialization is complete (or less\u001b[39;00m\n\u001b[0;32m    936\u001b[0m   \u001b[38;5;66;03m# interestingly an exception was raised) so we no longer need a lock.\u001b[39;00m\n\u001b[0;32m    937\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:763\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph \u001b[38;5;241m=\u001b[39m lifted_initializer_graph\n\u001b[0;32m    761\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_graph_deleter \u001b[38;5;241m=\u001b[39m FunctionDeleter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lifted_initializer_graph)\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_concrete_stateful_fn \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 763\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn\u001b[38;5;241m.\u001b[39m_get_concrete_function_internal_garbage_collected(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    764\u001b[0m         \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds))\n\u001b[0;32m    766\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvalid_creator_scope\u001b[39m(\u001b[38;5;241m*\u001b[39munused_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39munused_kwds):\n\u001b[0;32m    767\u001b[0m   \u001b[38;5;124;03m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3050\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m   args, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3049\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m-> 3050\u001b[0m   graph_function, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_define_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3051\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3444\u001b[0m, in \u001b[0;36mFunction._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3440\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_define_function_with_shape_relaxation(\n\u001b[0;32m   3441\u001b[0m       args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0;32m   3443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mmissed\u001b[38;5;241m.\u001b[39madd(call_context_key)\n\u001b[1;32m-> 3444\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_graph_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3445\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_cache\u001b[38;5;241m.\u001b[39mprimary[cache_key] \u001b[38;5;241m=\u001b[39m graph_function\n\u001b[0;32m   3447\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function, filtered_flat_args\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:3279\u001b[0m, in \u001b[0;36mFunction._create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3274\u001b[0m missing_arg_names \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   3275\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (arg, i) \u001b[38;5;28;01mfor\u001b[39;00m i, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(missing_arg_names)\n\u001b[0;32m   3276\u001b[0m ]\n\u001b[0;32m   3277\u001b[0m arg_names \u001b[38;5;241m=\u001b[39m base_arg_names \u001b[38;5;241m+\u001b[39m missing_arg_names\n\u001b[0;32m   3278\u001b[0m graph_function \u001b[38;5;241m=\u001b[39m ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[0m     \u001b[43mfunc_graph_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc_graph_from_py_func\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_function\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3282\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_signature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3286\u001b[0m \u001b[43m        \u001b[49m\u001b[43mautograph_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_autograph_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3287\u001b[0m \u001b[43m        \u001b[49m\u001b[43marg_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43marg_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3288\u001b[0m \u001b[43m        \u001b[49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverride_flat_arg_shapes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3289\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapture_by_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_capture_by_value\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m   3290\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_function_attributes,\n\u001b[0;32m   3291\u001b[0m     function_spec\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_spec,\n\u001b[0;32m   3292\u001b[0m     \u001b[38;5;66;03m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[39;00m\n\u001b[0;32m   3293\u001b[0m     \u001b[38;5;66;03m# scope. This is not the default behavior since it gets used in some\u001b[39;00m\n\u001b[0;32m   3294\u001b[0m     \u001b[38;5;66;03m# places (like Keras) where the FuncGraph lives longer than the\u001b[39;00m\n\u001b[0;32m   3295\u001b[0m     \u001b[38;5;66;03m# ConcreteFunction.\u001b[39;00m\n\u001b[0;32m   3296\u001b[0m     shared_func_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   3297\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph_function\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:999\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    996\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    997\u001b[0m   _, original_func \u001b[38;5;241m=\u001b[39m tf_decorator\u001b[38;5;241m.\u001b[39munwrap(python_func)\n\u001b[1;32m--> 999\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m python_func(\u001b[38;5;241m*\u001b[39mfunc_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfunc_kwargs)\n\u001b[0;32m   1001\u001b[0m \u001b[38;5;66;03m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[39;00m\n\u001b[0;32m   1002\u001b[0m \u001b[38;5;66;03m# TensorArrays and `None`s.\u001b[39;00m\n\u001b[0;32m   1003\u001b[0m func_outputs \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mmap_structure(convert, func_outputs,\n\u001b[0;32m   1004\u001b[0m                                   expand_composites\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:672\u001b[0m, in \u001b[0;36mFunction._defun_with_scope.<locals>.wrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m default_graph\u001b[38;5;241m.\u001b[39m_variable_creator_scope(scope, priority\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m):  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    669\u001b[0m   \u001b[38;5;66;03m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[39;00m\n\u001b[0;32m    670\u001b[0m   \u001b[38;5;66;03m# the function a weak reference to itself to avoid a reference cycle.\u001b[39;00m\n\u001b[0;32m    671\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(compile_with_xla):\n\u001b[1;32m--> 672\u001b[0m     out \u001b[38;5;241m=\u001b[39m weak_wrapped_fn()\u001b[38;5;241m.\u001b[39m__wrapped__(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    673\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:986\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[0;32m    985\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 986\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[0;32m    987\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420 call\n        return self._run_internal_graph(\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\lukec\\miniconda3\\envs\\tf\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:215 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer gru_2 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (20, 250)\n"
     ]
    }
   ],
   "source": [
    "#fitting the model (please god please work please)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "epochs = 10000\n",
    "batch_size = 20\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    #generate some data\n",
    "    X1, X2, Y = get_dataset(batch_size, MAX_SEQ_LEN)\n",
    "\n",
    "    #train the network\n",
    "    loss = model.train_on_batch(X1, Y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch {i+1}/{epochs}: LOSS={loss[0]}    ACC={loss[1]}\")\n",
    "\n",
    "print(\"Finished\")\n",
    "model.save(\"SpongeBobLSTM.h5\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e193c75",
   "metadata": {},
   "source": [
    "<h1>Problem #4: Testing the model</h1>\n",
    "<br></br>\n",
    "<p>Now that I have a trainable model, I need a way of testing it's capabilities outside of a graph. My goal is to create a program that takes a prompt, tokenizes and processes the prompt, and then passes it through the model until the model generates a story as big as what it can take as input (if that makes any sense). Hopefully the code should explain itself for this one as I'll be using a lot of the code I already written.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c0fbb8ed",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-24T05:26:04.618053Z",
     "start_time": "2023-08-24T05:25:45.032669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter prompt for episode: spongebob kills his friend\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b929eb19f33473292552ca88c2ad0e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/245 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "migger the episode begins with . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . \n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm as log_progress\n",
    "\n",
    "#how many tokens to take from the actual dataset \n",
    "start_seq_len = 5\n",
    "\n",
    "prompt = input(\"Enter prompt for episode: \")\n",
    "prompt = prompt.lower()\n",
    "\n",
    "#tokenize the prompt if it's short enough\n",
    "if len(prompt.split(' ')) < MAX_SEQ_LEN:\n",
    "    tokens = tokenize(prompt)\n",
    "    ep_prompt = []\n",
    "    \n",
    "    for i in tokens:\n",
    "        if i in lib:\n",
    "            ep_prompt.append(lib.index(i))\n",
    "        else:\n",
    "            ep_prompt.append(0)\n",
    "            \n",
    "    for i in range(MAX_SEQ_LEN-len(ep_prompt)):\n",
    "        ep_prompt.append(0)\n",
    "        \n",
    "    ep_prompt = np.array(ep_prompt)\n",
    "    ep_prompt = np.reshape(ep_prompt, (1, ep_prompt.shape[0]))\n",
    "    \n",
    "    #create a temp array to hold the previous tokens of the generated story\n",
    "    episode = np.zeros((1, MAX_SEQ_LEN))\n",
    "    episode[0][0:start_seq_len] = random.choice(tokenized)[0:start_seq_len]\n",
    "    \n",
    "    episode[0][0] = random.randrange(0, len(lib)) #start with a random token\n",
    "    \n",
    "    counter = start_seq_len\n",
    "    for i in log_progress(range(MAX_SEQ_LEN-start_seq_len)):\n",
    "        next_token = model.predict([episode, ep_prompt], verbose=0)\n",
    "        episode[0][counter] = next_token[0].argmax()\n",
    "        counter += 1\n",
    "        \n",
    "    #finally print the finished story\n",
    "    episode_script = ''\n",
    "    for i in episode[0]:\n",
    "        episode_script += lib[int(i)] + ' '\n",
    "        \n",
    "    print(episode_script)\n",
    "else:\n",
    "    print(\"Prompt is too long!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407aed4",
   "metadata": {},
   "source": [
    "<h2>Some notes:</h2>\n",
    "<ol>\n",
    "    <li>Embedding layer might not be that useful. I'm just trying to understand and generate sequences, not get the AI to understand the difference between words.</li>\n",
    "    <li>If embedding layers are useless, switching to a character based tokenization might be better than the word based tokenization method I'm using now.</li>\n",
    "    <li>The AI currently does not seem to understand any differences. I've changed the architecture of the model several times with no change in it's performance, so I'm going to guess the problem lies with how I'm feeding and prepping the data.</li>\n",
    "</ol>\n",
    "<br></br>\n",
    "<h3>Performance notes after changing data to character-based tokenization:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa52b1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
