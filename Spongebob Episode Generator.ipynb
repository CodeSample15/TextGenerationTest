{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8ecbc4f",
   "metadata": {},
   "source": [
    "<h1>I hate myself for attempting this but I'm bored</h1>\n",
    "<br></br>\n",
    "<h3>Let me explain myself because I feel this needs explaining:</h3>\n",
    "<p>Recently, the GPT model has dominated the text-generation realm of AI (as it should, it's fucking amazing). However, I want to see how far I can push the RNN model for text generation before it can't be pushed any further. Of course, conditional text generation has been done before in the past with RNN, and very well, BUT: I am stubborn as fuck and need to experiment by myself to see what I can do. Also I just finished working on a video game and need a not so overcomplicated project to suck me back into the world of AI. I'll document my findings here in this notebook so you can see the deterioration of my sanity.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daa4505f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:11:55.892547Z",
     "start_time": "2023-09-06T23:11:53.996776Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import glob\n",
    "from time import sleep\n",
    "\n",
    "from bs4 import BeautifulSoup as bs4\n",
    "import requests\n",
    "\n",
    "DESC_FILE = 'ep_descs.pkl'\n",
    "TRANS_FILE = 'C:/Users/lukec/Documents/Datasets/SpongeBob_SquarePants_Transcripts/'\n",
    "\n",
    "MAX_SEQ_LEN = 40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6be69ec7",
   "metadata": {},
   "source": [
    "<h2>Problem #1: web scraping</h2>\n",
    "<br></br>\n",
    "<p>In order to get the generator to generate scripts from a prompt, I need to get descriptions of each episode (which will be the prompts of this generator). Fortunately, this data can be found on the Spongebob Wiki. Unfortunately, this means I have to do some mother-fucking web scraping. I'll try explain what I'm doing with comments but to be honest, I'm going to give up on that pretty quickly. Sorry in advance for my ugly python code</p>\n",
    "<p>For ease of use later on, I'll store the data (which will be kept in a dictionary) into a seperate file using python's pickle</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2417651",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-05T01:13:38.509634Z",
     "start_time": "2023-09-05T01:13:31.277824Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#running for each season of spongebob\n",
    "SEASONS = 13\n",
    "data = {}\n",
    "\n",
    "for season in range(1, SEASONS+1):\n",
    "    page = requests.get(f\"https://spongebob.fandom.com/wiki/Season_{season}#List_of_episodes\").content\n",
    "    \n",
    "    soup = bs4(page, 'html.parser')\n",
    "    table = soup.find_all('table', class_='general')[0]\n",
    "        \n",
    "    #let's get the title and descriptions of each episode\n",
    "    titles = table.find_all('td', style='text-align:left')\n",
    "    descriptions = table.find_all('td', colspan='4')\n",
    "    \n",
    "    for j in range(len(titles)):\n",
    "        title = titles[j].text\n",
    "        desc = descriptions[j].text\n",
    "        #getting rid of the quotation marks around the episode name with possibly the most python line of code ever written (then also do some other pre processing)\n",
    "        title = title[1:-2]\n",
    "        title = title.replace(' ', '')\n",
    "        title = title.lower()\n",
    "        \n",
    "        data[title] = desc #updating the dataset\n",
    "        \n",
    "#save the compiled data to a file\n",
    "with open(DESC_FILE, 'wb') as f:\n",
    "    pickle.dump(data, f)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceeadb8",
   "metadata": {},
   "source": [
    "<p>Alright, that wasn't as bad as I thought it would be. I ended up getting the script to work on the third or fourth time. Luckily the spongebob wiki doesn't require javascript to load its shit, so I didn't have to boot up selenium to get this scraper to work. Come to think of it, this was probably the best experience I've had writing a web scraper. Nice</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e18a47c",
   "metadata": {},
   "source": [
    "<h1>Problem #2: bringing both datasets together</h1>\n",
    "<br></br>\n",
    "<p>Now that I have all the data I need, I need to compile everything into a single dataset that I can use to train the network with. This means for the transcripts, I have to clean, tokenize, and pad everything. For the episode descriptions, I need to combine the descriptions with the correct transcripts.</p>\n",
    "<p>Some things to consider: RNN models are notoriously bad at generating long sequences of text. Should I set the sampling size lower so that the model performs better? Or should I keep it high so I can really test the limits of this type of model. Hmmmm.... I guess I'll figure it out as I go, but I'm pretty sure I'm going to have to set the sampling size to be smaller. I'll start with the full transcripts and go from there. I think the main reason I want to do this is because I already know how bad LSTM is at performing this task, but I haven't really tested the GRU layer that much and am curious as to how it performs. I think I'll end up training two models.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de593a2b",
   "metadata": {},
   "source": [
    "<h3>Quick side note:</h3>\n",
    "<p>I want to really quickly go over in text what I want the architecture of the model to be (inputs and outputs for now, I'll go into more detail when I build the model). For starters, the model will have two inputs: X1 and X2. They will store the previous text in the transcript and the prompt, respectively. The output will be a one-hot encoding of the next token to be added to the sequence. My hope with using the two seperate inputs for the model will help it perform better when generating scripts that are more relevant to the provided episode prompt. Again, GPT models handle this problem with ease, but when it comes to RNNs, I want to make sure the model remembers what it should be generating</p>\n",
    "<p>I'll go with this design for as long as possible, so hopefully I don't lose my shit trying to keep this plan from failing. I have to remember that this is not going to work the first time cuz I have no clue what the fuck I'm doing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc50dd7d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:12:05.673822Z",
     "start_time": "2023-09-06T23:12:01.118064Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 items failed to load\n",
      "317 items loaded!\n"
     ]
    }
   ],
   "source": [
    "#let's start with the basics: loading the data from disk\n",
    "dataset = {}\n",
    "\n",
    "descs = {}\n",
    "with open(DESC_FILE, 'rb') as f:\n",
    "    descs = pickle.load(f)\n",
    "\n",
    "failed_items = 0\n",
    "successful_items = 0\n",
    "    \n",
    "for file in glob.glob(TRANS_FILE + '*.txt'):\n",
    "    ep_title = file.split('\\\\')[1].split('.')[0]\n",
    "    ep_title = ep_title.lower()\n",
    "    \n",
    "    #try to access the description of the episode, if it's not on file, just ignore it\n",
    "    try:\n",
    "        desc = descs[ep_title]\n",
    "        \n",
    "        lines = []\n",
    "        with open(file, 'r', encoding='utf-8') as f:\n",
    "            for line in f.readlines():\n",
    "                lines.append(line.lower())\n",
    "                #lines.extend('\\n')\n",
    "        \n",
    "        dataset[desc] = lines\n",
    "        successful_items += 1\n",
    "    except:\n",
    "        failed_items += 1\n",
    "        continue\n",
    "\n",
    "print(f\"{failed_items} items failed to load\")\n",
    "print(f\"{successful_items} items loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1f6959a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:13:39.543141Z",
     "start_time": "2023-09-06T23:12:07.070018Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data...\n",
      "Finished\n"
     ]
    }
   ],
   "source": [
    "#alright, now let's process and organize this data (spaghetti code incoming)\n",
    "print(\"Processing data...\")\n",
    "\n",
    "pretokenized = []\n",
    "pretokenizedDescs = []\n",
    "\n",
    "def tokenize(line):\n",
    "    #not even going to explain this mess. Just know this cleans the data. Don't currently know a better way of doing this and it's killing me inside. \n",
    "    tokens = []\n",
    "    for char in line:\n",
    "        tokens.append(char)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_words(line):\n",
    "    #not even going to explain this mess. Just know this cleans the data. Don't currently know a better way of doing this and it's killing me inside. \n",
    "    tokens = []\n",
    "    for word in line.split(' '):\n",
    "        newword = \"\"\n",
    "        for char in word:\n",
    "            if not char.isalpha():\n",
    "                if newword != \"\":\n",
    "                    tokens.append(newword)\n",
    "                newword = \"\"\n",
    "                tokens.append(char)\n",
    "            else:\n",
    "                newword += char\n",
    "        if(newword!=''):\n",
    "            tokens.append(newword)\n",
    "    return tokens\n",
    "\n",
    "for desc in dataset:\n",
    "    #clean the data\n",
    "    tempdata = [x for x in dataset[desc] if x != '\\n']\n",
    "    tokens = []\n",
    "    descTokens = []\n",
    "    \n",
    "    for line in tempdata:\n",
    "        line += \"\\n\"\n",
    "        tokens.extend(tokenize_words(line))\n",
    "        \n",
    "    pretokenizedDescs.append(tokenize_words(desc.lower()))\n",
    "    #removing unecessary tokens\n",
    "    pretokenizedDescs[-1].pop(pretokenizedDescs[-1].index('\\n'))\n",
    "    \n",
    "    pretokenized.append(tokens)\n",
    "    \n",
    "#now let's tokenize this sucker and then move on to building the dataset to be fed to the network.\n",
    "lib = [''] #library of tokens\n",
    "tokenized = [] #dataset tokenized\n",
    "tokenizedDescs = [] #same thing but descriptions\n",
    "maxLen = 0 #for padding later down the road\n",
    "maxDescLen = 0 #same thing but for descriptions\n",
    "\n",
    "#first the transcripts (calculating the maximum sequence length and then tokenizing the dataset using the lib array)\n",
    "for i in pretokenized:\n",
    "    if len(i) > maxLen:\n",
    "        maxLen = len(i)\n",
    "    for j in i:\n",
    "        if j not in lib:\n",
    "            lib.append(j)\n",
    "            \n",
    "for i in pretokenized:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(lib.index(j))\n",
    "    tokenized.append(temp)\n",
    "    \n",
    "#now the descriptions\n",
    "for i in pretokenizedDescs:\n",
    "    if len(i) > maxDescLen:\n",
    "        maxDescLen = len(i)\n",
    "    for j in i: #continuing to build upon the library just in case there are some tokens that didn't make the list yet\n",
    "        if j not in lib:\n",
    "            lib.append(j)\n",
    "            \n",
    "for i in pretokenizedDescs:\n",
    "    temp = []\n",
    "    for j in i:\n",
    "        temp.append(lib.index(j))\n",
    "    tokenizedDescs.append(temp)\n",
    "    \n",
    "#now we need to pad the tokenized descriptions since that won't happen automatically when the dataset it built\n",
    "for i, n in enumerate(tokenizedDescs):\n",
    "    for j in range(maxDescLen-len(n)):\n",
    "        tokenizedDescs[i].append(0) #padding (0 token represents and empty string in the lib array)\n",
    "\n",
    "print(\"Finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce3b6b91",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:17:44.672183Z",
     "start_time": "2023-09-06T23:17:44.654215Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "import random\n",
    "\n",
    "#smallest length a sequence can be\n",
    "MIN_LEN = MAX_SEQ_LEN\n",
    "\n",
    "X1 = [] #ep descriptions\n",
    "X2 = [] #prev tokens\n",
    "Y = []  #ep transcipts\n",
    "\n",
    "#helper to generate random batches of data from the main dataset\n",
    "#loading the whole dataset to memory at one time on my 16GB of ram would fry my computer pretty quickly. Let's avoid that\n",
    "def get_dataset(batch_size, size=MAX_SEQ_LEN):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    Y = []\n",
    "    for batch in range(batch_size):\n",
    "        rand_trans = random.randrange(MIN_LEN, len(tokenized)) #get a random transcript to get information from\n",
    "        rand_start = random.randrange(0, len(tokenized[rand_trans])-size)\n",
    "        \n",
    "        temp = tokenized[rand_trans][rand_start:rand_start+size]\n",
    "\n",
    "        X1.append(np.array(temp))\n",
    "        X2.append(tokenizedDescs[rand_trans])\n",
    "        Y.append(tokenized[rand_trans][rand_start+size])\n",
    "        \n",
    "    X1 = np.array(X1, dtype='float32')\n",
    "    X2 = np.array(X2, dtype='float32')\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    Y = to_categorical(Y, num_classes=len(lib))\n",
    "    return X1, X2, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3f7ac2",
   "metadata": {},
   "source": [
    "<h1>Problem #3: Building the model</h1>\n",
    "<br></br>\n",
    "<p>Gettting the data prepped is always the most annoying and difficult part. This time (for me at least) was no exception. I spent a good 2 hours writing the above cells, and now I'm ready to move on. Luckily, with everything put into place, it should IN THEORY be easy to set this model up.</p>\n",
    "<p>I already went over the plan in an above cell, so I won't explain myself again. This is already a stupid idea, I'm just going to roll with it at this point. I'm going to start with one GRU layer for each input layer, and then move on from there depending on how the model performs. I'll make sure to update this cell if I decide to change the model design drastically (which I most likely will do). I also plan on comparing the difference in performance between a LSTM model and a GRU model, so I'll document my findings later down the line. I feel like GRU will perform better, but I'll test both models just to be sure.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e583fb19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:17:47.457059Z",
     "start_time": "2023-09-06T23:17:47.431099Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GRU, LSTM, Dropout, BatchNormalization, concatenate, TimeDistributed, Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def GRUModel():\n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN))\n",
    "    \n",
    "    x = Embedding(len(lib), 20, input_length=1)(x_in)\n",
    "    \n",
    "    x = GRU(128)(x)\n",
    "    \n",
    "    x = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    #descriptions input\n",
    "    y_in = Input(shape=(maxDescLen))\n",
    "    \n",
    "    y = Embedding(len(lib), 20, input_length=1)(y_in)\n",
    "    \n",
    "    y = GRU(64)(y)\n",
    "    \n",
    "    y = Model(inputs=y_in, outputs=y)\n",
    "    \n",
    "    #models combined\n",
    "    z = concatenate([x, y])\n",
    "    \n",
    "    z = Dense(128)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    \n",
    "    z = Dense(len(lib), activation='softmax')(z)\n",
    "    \n",
    "    z = Model(inputs=[x_in, y_in], outputs=z)\n",
    "    return z\n",
    "\n",
    "def LSTMModel():\n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN))\n",
    "    \n",
    "    x = Embedding(len(lib), 20, input_length=1)(x_in)\n",
    "    \n",
    "    x = LSTM(128)(x)\n",
    "    \n",
    "    x = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    #descriptions input\n",
    "    y_in = Input(shape=(maxDescLen))\n",
    "    \n",
    "    y = Embedding(len(lib), 20, input_length=1)(y_in)\n",
    "    \n",
    "    y = LSTM(64)(y)\n",
    "    \n",
    "    y = Model(inputs=y_in, outputs=y)\n",
    "    \n",
    "    #models combined\n",
    "    z = concatenate([x.output, y.output])\n",
    "    \n",
    "    z = Dense(128)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    \n",
    "    z = Dense(len(lib), activation='softmax')(z)\n",
    "    \n",
    "    z = Model(inputs=[x_in, y_in], outputs=z)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bb8646ef",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:17:50.358259Z",
     "start_time": "2023-09-06T23:17:48.774354Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_4 (InputLayer)            [(None, 39)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 40, 20)       357340      input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 39, 20)       357340      input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 128)          76288       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 64)           21760       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 192)          0           lstm[0][0]                       \n",
      "                                                                 lstm_1[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 128)          24704       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 128)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 128)          0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 17867)        2304843     dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,142,275\n",
      "Trainable params: 3,142,275\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = LSTMModel()\n",
    "\n",
    "opt = Adam(learning_rate=0.0003)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5921be10",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:22:05.505508Z",
     "start_time": "2023-09-06T23:17:53.145994Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000: LOSS=5.513247489929199    ACC=0.09375\n",
      "Finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x15cbe1b0220>,\n",
       " <matplotlib.lines.Line2D at 0x15cbe1b0340>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#fitting the model (please god please work please)\n",
    "from IPython.display import clear_output\n",
    "\n",
    "epochs = 1000\n",
    "batch_size = 128\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    #generate some data\n",
    "    X1, X2, Y = get_dataset(batch_size, MAX_SEQ_LEN)\n",
    "\n",
    "    #train the network\n",
    "    loss = model.train_on_batch([X1, X2], Y)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "    print(f\"Epoch {i+1}/{epochs}: LOSS={loss[0]}    ACC={loss[1]}\")\n",
    "\n",
    "print(\"Finished\")\n",
    "model.save(\"SpongeBobLSTM.h5\")\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e193c75",
   "metadata": {},
   "source": [
    "<h1>Problem #4: Testing the model</h1>\n",
    "<br></br>\n",
    "<p>Now that I have a trainable model, I need a way of testing it's capabilities outside of a graph. My goal is to create a program that takes a prompt, tokenizes and processes the prompt, and then passes it through the model until the model generates a story as big as what it can take as input (if that makes any sense). Hopefully the code should explain itself for this one as I'll be using a lot of the code I already written.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "262ab4f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:22:37.669232Z",
     "start_time": "2023-09-06T23:22:15.752441Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter prompt for episode: Spongebob finds a hidden sea shell and sells it to patrick\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f257c38d7b24afead1c079d0149dba1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=50.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[ pan over live - action houses ] \n",
      " \n",
      " french narrator : we take you now to encino , california , where we find the president of the spongebob squarepants fan club . . flipping burgers ? ! \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm as log_progress\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def shift(arr, new=0.0):\n",
    "    for i in range(1, len(arr[0])):\n",
    "        arr[0][i-1] = arr[0][i]\n",
    "    arr[0][-1] = new\n",
    "\n",
    "model = load_model(\"SpongebobLSTM.h5\")\n",
    "\n",
    "#how many tokens to take from the actual dataset \n",
    "start_seq_len = MAX_SEQ_LEN\n",
    "EP_LEN = 50 #how many times the program should run\n",
    "\n",
    "prompt = input(\"Enter prompt for episode: \")\n",
    "prompt = prompt.lower()\n",
    "\n",
    "#tokenize the prompt if it's short enough\n",
    "if len(prompt.split(' ')) < maxDescLen:\n",
    "    tokens = tokenize_words(prompt)\n",
    "    ep_prompt = []\n",
    "    \n",
    "    for i in tokens:\n",
    "        if i in lib:\n",
    "            ep_prompt.append(lib.index(i))\n",
    "        else:\n",
    "            ep_prompt.append(0)\n",
    "            \n",
    "    for i in range(maxDescLen-len(ep_prompt)):\n",
    "        ep_prompt.append(0)\n",
    "        \n",
    "    ep_prompt = np.array(ep_prompt)\n",
    "    ep_prompt = np.reshape(ep_prompt, (1, ep_prompt.shape[0]))\n",
    "    \n",
    "    #create a temp array to hold the previous tokens of the generated story\n",
    "    episode = []\n",
    "    \n",
    "    temp = np.zeros((1, MAX_SEQ_LEN))\n",
    "    temp[0][0:start_seq_len] = random.choice(tokenized)[0:start_seq_len]\n",
    "    \n",
    "    for i in range(start_seq_len):\n",
    "        episode.append(int(temp[0][i]))\n",
    "    \n",
    "    for i in log_progress(range(EP_LEN)):\n",
    "        next_token = model.predict([temp, ep_prompt], verbose=0)\n",
    "        episode.append(int(next_token[0].argmax()))\n",
    "        shift(temp, new=next_token[0].argmax())\n",
    "        \n",
    "    #finally print the finished story\n",
    "    episode_script = ''\n",
    "    for i in episode:\n",
    "        episode_script += lib[i] + ' '\n",
    "        \n",
    "    print(episode_script)\n",
    "else:\n",
    "    print(\"Prompt is too long!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9407aed4",
   "metadata": {},
   "source": [
    "<h2>Some notes:</h2>\n",
    "<ol>\n",
    "    <li>Embedding layer might not be that useful. I'm just trying to understand and generate sequences, not get the AI to understand the difference between words.</li>\n",
    "    <li>If embedding layers are useless, switching to a character based tokenization might be better than the word based tokenization method I'm using now.</li>\n",
    "    <li>The AI currently does not seem to understand any differences. I've changed the architecture of the model several times with no change in it's performance, so I'm going to guess the problem lies with how I'm feeding and prepping the data.</li>\n",
    "</ol>\n",
    "<br></br>\n",
    "<h3>Performance notes after changing data to character-based tokenization:</h3>\n",
    "<ul>\n",
    "    <li>Doesn't seem to be any change in performance, but I just started with this new strategy, so I'm going to put a little more work into it.</li>\n",
    "    <li>Not sure if it was the character based tokenization or the fact that the model isn't training on empty inputs anymore (no more 0 padding in training data), but the model is now actually performing better. I also simplified the model a bit to only take one input (previous tokens) in order to debug the lack of learning, which also could contribute to this boost in performance. I'm going to add back the second input with this new method and see if there's any changes.</li>\n",
    "    <li>It seems to be the fact that input data is no longer padded that the model is performing better than ever. I am going to try and go back to the old tokenization method to see if that will change anything.</li>\n",
    "</ul>\n",
    "\n",
    "<h3>Notes after changing back to word-based tokenization:</h3>\n",
    "<ul>\n",
    "    <li>I included the loss (blue) and accuracy (orange) graph below:</li>\n",
    "    <img src='Graph1.png'>\n",
    "    <li>As you can see, the training is unstable as fuck, but it is slowly getting better over time, which is good. I'm going to add back the second input as well as testing the model with what it can currently produce. After making those changes, I got this graph:</li>\n",
    "    <img src='Graph2.png'>\n",
    "    <li>No idea what the fuck happened here. Amazing how the loss started so high at first, but that might be just from the extra neurons I threw into the model. To make sure the first input isn't the only part of the model training, I'll add some dropout to both inputs and also mess with the Embedding layers for both inputs.</li>\n",
    "    <li>So this is disappointing:</li>\n",
    "    <img src='Graph3.png'>\n",
    "    <li>It seems that the model is no longer improving after training some more... This is annoying.</li>\n",
    "</ul>\n",
    "<br></br>\n",
    "<h3>Let's talk validation data:</h3>\n",
    "<h4>(and why this approach I'm taking is terrible)</h4>\n",
    "<p>Many people who aren't trying to kill their careers in ML will use validation data to make sure their model is actually learning rather than memorizing a dataset. This is smart. It introduces new data to the model that it hasn't seen before and allows for programmers to see if their model is overfitting or not. So why am I not using validation data?</p>\n",
    "<br></br>\n",
    "<p>Well, the answer is simple. <b>I'm stupid and stubborn</b>. I am feeding the network very little data. You might say \"there's so much training data in the episode transcripts, you have no excuse\", and you're right, but I am also feeding to the network the descriptions for each episode, which is very little information to the network. There's only so many descriptions, and while I can split up the transcripts to get more training data, I can't split up sequences as little as descriptions, especially when I need the whole desc sequence to actually represent a description for an episode.</p>\n",
    "<p>I mean, if I'm being honest it's not the worse solution. I mean, think about it: In theory this will produced an overfitted model that generates infinite episodes given the topic of any existing spongebob episode. In theory, at least.</p>\n",
    "<br></br>\n",
    "<h3>A slightly better approach:</h3>\n",
    "<h4>(Apart from using something like the GPT model)</h4>\n",
    "<p>Transfer learning. Use a model that already has a fully trained Embedding and LSTM/GRU layer and slap it to the top of my model. This would give the model a giant boost, but I'm still not sure how well it would do. Might experiment with this in the future...</p>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<h1>Results:</h1>\n",
    "<h4>(Prompt used: Spongebob finds a rare sea shell)</h4>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<h3>GRU Model:</h3>\n",
    "<p>After 1000 epochs:</p>\n",
    "<img src='Results1.png'>\n",
    "<p>After 2000 epochs: (Seems like it's getting better(?))</p>\n",
    "<img src='Results2.png'>\n",
    "<p>After 3000 epochs:</p>\n",
    "<p>No image here because the output is just a bunch of whitespace interrupted by a lot of periods and the occasional \"squidward\"...</p>\n",
    "<br></br>\n",
    "<p>Damn it</p>\n",
    "<br>After 4000 epochs:</br>\n",
    "<p>...It's just whitespace</p>\n",
    "<p><b>FUCK</b></p>\n",
    "<br></br>\n",
    "<p>Well, hopefully I can maybe salvage this experiment by getting some interesting results with LSTM to compare to GRU.</p>\n",
    "<br></br>\n",
    "\n",
    "<h3>LSTM Model:</h3>\n",
    "<p>After 1000 epochs:</p>\n",
    "<img src='Results3.png'>\n",
    "<p>After 2000 epochs: (noticing a trend here)</p>\n",
    "<img src='Results4.png'>\n",
    "<p>After 3000 epochs:</p>\n",
    "<img src='Results5.png'>\n",
    "<p>After 4000 epochs:</p>\n",
    "<p>Whitespace again...</p>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<p>Alright, back to the drawing board.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ed2b2f",
   "metadata": {},
   "source": [
    "<h2>Notes about the old method:</h2>\n",
    "<ul>\n",
    "    <li>It overfits like crazy.</li>\n",
    "    <li>When I refer to an \"epoch\", it's not a true epoch, but rather one batch of data going through the model. I can't be bothered to change this, so from now on that's what I'm referring to when I say something like \"one epoch of training\".</li>\n",
    "</ul>\n",
    "\n",
    "<br></br>\n",
    "\n",
    "<h1>New Plan:</h1>\n",
    "<p>Let's ditch the old plan of providing a description for each episode generated. That's going to introduce a lot of headaches in terms of prepping the data and somehow not overfitting the model.</p>\n",
    "<p>Alright, cool, now the whole experiment can be shut down. However, I still want to make something from this failed project so that I don't feel like this was a waste of time (which it probably was but who cares, it's fun). My new plan is this:</p>\n",
    "<ol>\n",
    "    <li>Ditch the description input side of the model.</li>\n",
    "    <li>Make a simple, generic RNN model using either GRU or LSTM to generate sequences of text that will hopefully resemble an episode of spongebob.</li>\n",
    "    <li>To provide a way for the user to have at least <i>some</i> control over what the program generates, the user will be able to input the first n number of tokens for the model to start generating with.</li>\n",
    "    <li>Also, since I want to use the descriptions in some way, pass in either a random pre-determined description or a vector representing a specific spongebob episode (one hot encoded). My hope for this is this will allow users to guide the output of the model to follow a theme similar to a specific episode of spongebob.</li>\n",
    "</ol>\n",
    "\n",
    "<br></br>\n",
    "<p>Hopefully with these changes, the model will perform better while still offering a little control to the user.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d0d7ba",
   "metadata": {},
   "source": [
    "<h1>Fuck-up #1:</h1>\n",
    "<p>I'm so dumb. When making the model, I concatenated the inputs of the two input models, rather than their outputs. This lead to the GRU and LSTM layers to <b>NOT EVEN RUN</b> when training. Re-running the training algorithm now to see if I can fix my fuck up.</p>\n",
    "\n",
    "<h1>After fixing the fuck-up:</h1>\n",
    "<p>My previous observations and notes are still correct, and the model is still producing whitespace as output, only it reaches this point a lot faster in the training process, which sucks but at least I didn't write a ton of notes for nothing.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01648cb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:14:01.373898Z",
     "start_time": "2023-09-06T23:13:51.499366Z"
    }
   },
   "outputs": [],
   "source": [
    "#get our old imports\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Dense, GRU, LSTM, Dropout, BatchNormalization, concatenate, TimeDistributed, Flatten\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tqdm.notebook import tqdm as log_progress\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "72978487",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:39:55.388184Z",
     "start_time": "2023-09-06T23:39:55.365879Z"
    }
   },
   "outputs": [],
   "source": [
    "#smallest length a sequence can be\n",
    "MIN_LEN = MAX_SEQ_LEN\n",
    "\n",
    "X1 = [] #ep descriptions\n",
    "X2 = [] #prev tokens\n",
    "Y = []  #ep transcipts\n",
    "\n",
    "#Let's make the new model\n",
    "def V2Model():\n",
    "    #transcript input\n",
    "    x_in = Input(shape=(MAX_SEQ_LEN))\n",
    "    \n",
    "    x = Embedding(len(lib), 20, input_length=1)(x_in)\n",
    "    \n",
    "    x = GRU(128, return_sequences=True)(x)\n",
    "    x = GRU(128)(x)\n",
    "    \n",
    "    x = Model(inputs=x_in, outputs=x)\n",
    "    \n",
    "    #one hot encoded of what episode theme should be used (might not use this)\n",
    "    y_in = Input(shape=(successful_items)) #successful items is the number of episodes successfully loaded from the dataset\n",
    "    \n",
    "    y = Dense(128)(y_in)\n",
    "    y = LeakyReLU()(y)\n",
    "    y = Dropout(0.2)(y)\n",
    "    \n",
    "    y = Model(inputs=y_in, outputs=y)\n",
    "    \n",
    "    #combined models\n",
    "    z = concatenate([x.output, y.output])\n",
    "    \n",
    "    z = Dense(128)(z)\n",
    "    z = LeakyReLU()(z)\n",
    "    z = Dropout(0.2)(z)\n",
    "    \n",
    "    z = Dense(len(lib), activation='softmax')(z)\n",
    "    z = Model(inputs=[x_in, y_in], outputs=z)\n",
    "    return z\n",
    "    \n",
    "\n",
    "#next we need a new way of preparing datasets for the new network\n",
    "def get_dataset(batch_size):\n",
    "    X1 = []\n",
    "    X2 = []\n",
    "    Y = []\n",
    "    for batch in range(batch_size):\n",
    "        rand_trans = random.randrange(0, len(tokenized)) #get a random transcript to get information from\n",
    "        rand_start = random.randrange(0, len(tokenized[rand_trans])-MAX_SEQ_LEN)\n",
    "        \n",
    "        temp = tokenized[rand_trans][rand_start:rand_start+MAX_SEQ_LEN]\n",
    "        \n",
    "        #one hot encoding of which episode theme should the AI use. \n",
    "        one_hot = np.zeros((successful_items))\n",
    "        one_hot[rand_trans] = 1\n",
    "        \n",
    "        X1.append(np.array(temp))\n",
    "        X2.append(one_hot)\n",
    "        Y.append(tokenized[rand_trans][rand_start+MAX_SEQ_LEN])\n",
    "        \n",
    "    X1 = np.array(X1, dtype='float32')\n",
    "    X2 = np.array(X2, dtype='float32')\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    Y = to_categorical(Y, num_classes=len(lib))\n",
    "    return X1, X2, Y\n",
    "\n",
    "#and finally, we need a new training loop\n",
    "#it's not going to be much different from the old training model, but I feel like it's just more organized to have a method\n",
    "#for everything new I'm adding / changing\n",
    "def train(model, epochs, batch_size, name=\"Model.h5\"):\n",
    "    losses = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        #generate some data\n",
    "        X1, X2, Y = get_dataset(batch_size)\n",
    "\n",
    "        #train the network\n",
    "        loss = model.train_on_batch([X1, X2], Y)\n",
    "        losses.append(loss)\n",
    "\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Epoch {i+1}/{epochs}: LOSS={loss[0]}    ACC={loss[1]}\")\n",
    "\n",
    "    print(\"Finished\")\n",
    "    model.save(name)\n",
    "    plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e290fbd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:39:57.970100Z",
     "start_time": "2023-09-06T23:39:57.639107Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_25\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_19 (InputLayer)           [(None, 40)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_20 (InputLayer)           [(None, 317)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_10 (Embedding)        (None, 40, 20)       357340      input_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 128)          40704       input_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_8 (GRU)                     (None, 40, 128)      57600       embedding_10[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 128)          0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "gru_9 (GRU)                     (None, 128)          99072       gru_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 128)          0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 256)          0           gru_9[0][0]                      \n",
      "                                                                 dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 128)          32896       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 128)          0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 128)          0           leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 17867)        2304843     dropout_16[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 2,892,455\n",
      "Trainable params: 2,892,455\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = V2Model()\n",
    "\n",
    "opt = Adam(learning_rate=0.0003)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d177680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-06T23:44:11.926374Z",
     "start_time": "2023-09-06T23:40:07.858980Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 826/2000: LOSS=6.005267143249512    ACC=0.046875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-6abfad826711>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m128\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNAME\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-31-9c79ae67510f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, epochs, batch_size, name)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m         \u001b[1;31m#train the network\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mX1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1819\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1820\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1821\u001b[1;33m       iterator = data_adapter.single_batch_iterator(self.distribute_strategy, x,\n\u001b[0m\u001b[0;32m   1822\u001b[0m                                                     \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1823\u001b[0m                                                     class_weight)\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36msingle_batch_iterator\u001b[1;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_make_class_weight_map_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1618\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1619\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1621\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    484\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    485\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 486\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    487\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    488\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    694\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    698\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 700\u001b[1;33m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_options\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[1;31m# Store dataset reference to ensure that dataset is alive when this iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_apply_options\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    459\u001b[0m           (options.experimental_optimization.apply_default_optimizations  # pylint: disable=g-bool-id-comparison\n\u001b[0;32m    460\u001b[0m            is not False)):\n\u001b[1;32m--> 461\u001b[1;33m       dataset = _OptimizeDataset(dataset, graph_rewrites.enabled,\n\u001b[0m\u001b[0;32m    462\u001b[0m                                  \u001b[0mgraph_rewrites\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisabled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m                                  graph_rewrites.default, graph_rewrite_configs)\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, optimization_configs)\u001b[0m\n\u001b[0;32m   4888\u001b[0m         argument_dtype=dtypes.string)\n\u001b[0;32m   4889\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4890\u001b[1;33m     variant_tensor = gen_dataset_ops.optimize_dataset_v2(\n\u001b[0m\u001b[0;32m   4891\u001b[0m         \u001b[0minput_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optimizations_enabled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36moptimize_dataset_v2\u001b[1;34m(input_dataset, optimizations_enabled, optimizations_disabled, optimizations_default, output_types, output_shapes, optimization_configs, name)\u001b[0m\n\u001b[0;32m   4167\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4168\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4169\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   4170\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"OptimizeDatasetV2\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizations_enabled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4171\u001b[0m         \u001b[0moptimizations_disabled\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizations_default\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"output_types\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NAME = \"SpongeBobV2.h5\"\n",
    "EPOCHS = 2000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train(model, EPOCHS, BATCH_SIZE, name=NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be552f2b",
   "metadata": {},
   "source": [
    "<h1>New model performance notes:</h1>\n",
    "<p>Right away the model is performing a ton better than method #1. The loss is dropping while the accuracy is rising. This is a good sign as it shows that there is much less overfitting taking place. I still have to write a new script to properly test the model, but the graphs are looking much better so far, and I am less sad.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0912a782",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
